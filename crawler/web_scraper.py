# -*- coding: utf-8 -*-
"""
API í‚¤ ì—†ì´ ì‘ë™í•˜ëŠ” ì›¹ í¬ë¡¤ëŸ¬
Seleniumì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œ ì›¹í˜ì´ì§€ë¥¼ í¬ë¡¤ë§
"""
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import re
from typing import List, Dict
import json

class WebScraper:
    def __init__(self, headless=True):
        """
        Selenium ì›¹ë“œë¼ì´ë²„ ì´ˆê¸°í™”
        """
        chrome_options = Options()
        if headless:
            chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')

        self.driver = webdriver.Chrome(
            service=Service(ChromeDriverManager().install()),
            options=chrome_options
        )
        self.wait = WebDriverWait(self.driver, 10)

    def __del__(self):
        """ë“œë¼ì´ë²„ ì¢…ë£Œ"""
        if hasattr(self, 'driver'):
            self.driver.quit()

    def search_naver_pools(self, query="ìˆ˜ì˜ì¥", region="ì„œìš¸", max_results=50) -> List[Dict]:
        """
        ë„¤ì´ë²„ ì§€ë„ì—ì„œ ìˆ˜ì˜ì¥ ê²€ìƒ‰
        """
        print(f"\nğŸ” ë„¤ì´ë²„ ì§€ë„ ê²€ìƒ‰: {region} {query}")

        search_url = f"https://map.naver.com/p/search/{region} {query}"
        self.driver.get(search_url)
        time.sleep(3)

        pools = []

        try:
            # iframeìœ¼ë¡œ ì „í™˜
            self.driver.switch_to.frame('searchIframe')
            time.sleep(2)

            # ìŠ¤í¬ë¡¤í•˜ì—¬ ë” ë§ì€ ê²°ê³¼ ë¡œë“œ
            scroll_container = self.driver.find_element(By.CSS_SELECTOR, '.Ryr1F')

            for i in range(5):  # 5ë²ˆ ìŠ¤í¬ë¡¤
                self.driver.execute_script("arguments[0].scrollTop = arguments[0].scrollHeight", scroll_container)
                time.sleep(1)

            # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
            results = self.driver.find_elements(By.CSS_SELECTOR, '.CHC5F')

            print(f"  ì°¾ì€ ê²°ê³¼: {len(results)}ê°œ")

            for idx, result in enumerate(results[:max_results]):
                try:
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    name_elem = result.find_element(By.CSS_SELECTOR, '.TYaxT')
                    name = name_elem.text.strip()

                    # ì£¼ì†Œ
                    try:
                        address_elem = result.find_element(By.CSS_SELECTOR, '.LDgIH')
                        address = address_elem.text.strip()
                    except:
                        address = ""

                    # ì „í™”ë²ˆí˜¸
                    try:
                        phone_elem = result.find_element(By.CSS_SELECTOR, '.dry0B')
                        phone = phone_elem.text.strip()
                    except:
                        phone = ""

                    # í‰ì 
                    try:
                        rating_elem = result.find_element(By.CSS_SELECTOR, '.PXMot')
                        rating_text = rating_elem.text.strip()
                        rating = float(rating_text.split()[0]) if rating_text else None
                    except:
                        rating = None

                    pool_data = {
                        "name": name,
                        "address": address,
                        "phone": phone,
                        "rating": rating,
                        "source": "ë„¤ì´ë²„ì§€ë„_ì›¹í¬ë¡¤ë§",
                        "lat": None,  # ì¢Œí‘œëŠ” ìƒì„¸í˜ì´ì§€ì—ì„œ ì¶”ì¶œ ê°€ëŠ¥
                        "lng": None
                    }

                    pools.append(pool_data)

                    if (idx + 1) % 10 == 0:
                        print(f"  ì§„í–‰ì¤‘... {idx + 1}ê°œ ìˆ˜ì§‘")

                except Exception as e:
                    continue

            self.driver.switch_to.default_content()

        except Exception as e:
            print(f"  âš ï¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

        print(f"  âœ… ì´ {len(pools)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return pools

    def search_kakao_pools(self, query="ìˆ˜ì˜ì¥", region="ì„œìš¸", max_pages=3) -> List[Dict]:
        """
        ì¹´ì¹´ì˜¤ë§µì—ì„œ ìˆ˜ì˜ì¥ ê²€ìƒ‰
        """
        print(f"\nğŸ” ì¹´ì¹´ì˜¤ë§µ ê²€ìƒ‰: {region} {query}")

        search_url = f"https://map.kakao.com/?q={region}+{query}"
        self.driver.get(search_url)
        time.sleep(3)

        pools = []

        try:
            # ê²€ìƒ‰ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì´ë™
            results_tab = self.wait.until(
                EC.presence_of_element_located((By.CSS_SELECTOR, '#info\\.search\\.place\\.list'))
            )

            for page in range(1, max_pages + 1):
                print(f"  í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘...")

                # í˜„ì¬ í˜ì´ì§€ ê²°ê³¼ íŒŒì‹±
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                place_list = soup.select('.placelist > .PlaceItem')

                for item in place_list:
                    try:
                        # ì´ë¦„
                        name_elem = item.select_one('.head_item .tit_name .link_name')
                        name = name_elem.text.strip() if name_elem else ""

                        # ì£¼ì†Œ
                        address_elem = item.select_one('.info_item .addr p')
                        address = address_elem.text.strip() if address_elem else ""

                        # ì „í™”ë²ˆí˜¸
                        phone_elem = item.select_one('.info_item .contact_item .txt_contact')
                        phone = phone_elem.text.strip() if phone_elem else ""

                        # ì¹´í…Œê³ ë¦¬
                        category_elem = item.select_one('.head_item .subcategory')
                        category = category_elem.text.strip() if category_elem else ""

                        pool_data = {
                            "name": name,
                            "address": address,
                            "phone": phone,
                            "category": category,
                            "source": "ì¹´ì¹´ì˜¤ë§µ_ì›¹í¬ë¡¤ë§",
                            "lat": None,
                            "lng": None
                        }

                        pools.append(pool_data)

                    except Exception as e:
                        continue

                # ë‹¤ìŒ í˜ì´ì§€ ë²„íŠ¼ í´ë¦­
                if page < max_pages:
                    try:
                        next_button = self.driver.find_element(By.CSS_SELECTOR, f'#info\\.search\\.page\\.next')
                        if 'disabled' not in next_button.get_attribute('class'):
                            next_button.click()
                            time.sleep(2)
                        else:
                            break
                    except:
                        break

        except Exception as e:
            print(f"  âš ï¸ í¬ë¡¤ë§ ì˜¤ë¥˜: {e}")

        print(f"  âœ… ì´ {len(pools)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return pools

    def crawl_all_regions(self, regions=None) -> List[Dict]:
        """
        ì—¬ëŸ¬ ì§€ì—­ì˜ ìˆ˜ì˜ì¥ í¬ë¡¤ë§
        """
        if regions is None:
            regions = [
                "ì„œìš¸", "ê²½ê¸°", "ì¸ì²œ",
                "ë¶€ì‚°", "ëŒ€êµ¬", "ê´‘ì£¼", "ëŒ€ì „", "ìš¸ì‚°",
                "ê°•ì›", "ì¶©ë¶", "ì¶©ë‚¨", "ì „ë¶", "ì „ë‚¨", "ê²½ë¶", "ê²½ë‚¨", "ì œì£¼"
            ]

        all_pools = []
        seen = set()

        for region in regions:
            print(f"\n{'='*50}")
            print(f"ğŸ“ ì§€ì—­: {region}")
            print('='*50)

            # ë„¤ì´ë²„ í¬ë¡¤ë§
            try:
                naver_pools = self.search_naver_pools(query="ìˆ˜ì˜ì¥", region=region, max_results=30)

                for pool in naver_pools:
                    key = f"{pool['name']}_{pool['address']}"
                    if key not in seen:
                        seen.add(key)
                        all_pools.append(pool)
            except Exception as e:
                print(f"  âŒ ë„¤ì´ë²„ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")

            time.sleep(2)

            # ì¹´ì¹´ì˜¤ í¬ë¡¤ë§ì€ êµ¬ì¡°ê°€ ë³µì¡í•˜ì—¬ ì¼ë‹¨ ë„¤ì´ë²„ë§Œ ì§„í–‰
            # í•„ìš”ì‹œ ì¶”ê°€ êµ¬í˜„

        return all_pools

def save_to_json(data: List[Dict], filename="crawled_pools.json"):
    """ìˆ˜ì§‘ ë°ì´í„° JSON ì €ì¥"""
    with open(filename, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")

if __name__ == "__main__":
    print("="*60)
    print("ğŸŠ í•œêµ­ ìˆ˜ì˜ì¥ ì›¹ í¬ë¡¤ëŸ¬ ì‹œì‘")
    print("="*60)

    scraper = WebScraper(headless=False)  # ë¸Œë¼ìš°ì € ë³´ì´ê²Œ ì„¤ì •

    try:
        # ì„œìš¸ë§Œ í…ŒìŠ¤íŠ¸
        pools = scraper.search_naver_pools(query="ìˆ˜ì˜ì¥", region="ì„œìš¸", max_results=50)

        # JSON íŒŒì¼ë¡œ ì €ì¥
        save_to_json(pools, "seoul_pools.json")

        print(f"\nâœ… í¬ë¡¤ë§ ì™„ë£Œ! ì´ {len(pools)}ê°œ ìˆ˜ì˜ì¥ ìˆ˜ì§‘")

        # ìƒ˜í”Œ ì¶œë ¥
        if pools:
            print("\nğŸ“‹ ìƒ˜í”Œ ë°ì´í„°:")
            for i, pool in enumerate(pools[:3], 1):
                print(f"\n{i}. {pool['name']}")
                print(f"   ì£¼ì†Œ: {pool['address']}")
                print(f"   ì „í™”: {pool['phone']}")
                print(f"   í‰ì : {pool['rating']}")

    finally:
        del scraper
