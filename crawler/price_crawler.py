# -*- coding: utf-8 -*-
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from typing import Dict, Optional, List
import sqlite3
from urllib.parse import quote

class PoolPriceCrawler:
    def __init__(self):
        self.naver_client_id = "VDnVKXoA66gaC4cz3vzc"
        self.naver_client_secret = "XuMSFxDf35"
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })

    def find_pool_website(self, pool_name: str, address: str) -> Optional[str]:
        """ë„¤ì´ë²„ ê²€ìƒ‰ìœ¼ë¡œ ìˆ˜ì˜ì¥ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì°¾ê¸°"""
        try:
            # ë„¤ì´ë²„ ì›¹ê²€ìƒ‰ API ì‚¬ìš©
            search_query = f"{pool_name} {address.split()[1] if len(address.split()) > 1 else ''} ìˆ˜ì˜ì¥"

            response = requests.get(
                "https://openapi.naver.com/v1/search/webkr.json",
                headers={
                    "X-Naver-Client-Id": self.naver_client_id,
                    "X-Naver-Client-Secret": self.naver_client_secret
                },
                params={
                    "query": search_query,
                    "display": 5
                }
            )

            if response.status_code == 200:
                items = response.json().get("items", [])

                # ê³µì‹ ì‚¬ì´íŠ¸ë¡œ ë³´ì´ëŠ” URL ìš°ì„ ìˆœìœ„
                priority_domains = ['go.kr', 'or.kr', 'co.kr', 'com']
                exclude_keywords = ['blog', 'cafe', 'post', 'news', 'review']

                for item in items:
                    url = item.get("link", "")
                    title = item.get("title", "").lower()

                    # ë¸”ë¡œê·¸, ì¹´í˜, ë‰´ìŠ¤ ì œì™¸
                    if any(keyword in url.lower() for keyword in exclude_keywords):
                        continue

                    # ìˆ˜ì˜ì¥ ì´ë¦„ì´ íƒ€ì´í‹€ì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                    pool_name_clean = pool_name.replace(" ", "").lower()
                    title_clean = title.replace(" ", "").replace("<b>", "").replace("</b>", "")

                    if pool_name_clean[:4] in title_clean or title_clean[:4] in pool_name_clean:
                        return url

                # ìš°ì„ ìˆœìœ„ ë„ë©”ì¸ ì°¾ê¸°
                for domain in priority_domains:
                    for item in items:
                        url = item.get("link", "")
                        if domain in url and not any(kw in url for kw in exclude_keywords):
                            return url

                # ì²« ë²ˆì§¸ ê²°ê³¼ ë°˜í™˜ (ë¸”ë¡œê·¸ ë“± ì œì™¸)
                for item in items:
                    url = item.get("link", "")
                    if not any(keyword in url.lower() for keyword in exclude_keywords):
                        return url

            time.sleep(0.1)
            return None

        except Exception as e:
            print(f"âœ— ì›¹ì‚¬ì´íŠ¸ ì°¾ê¸° ì‹¤íŒ¨ ({pool_name}): {str(e)[:50]}")
            return None

    def extract_price_from_text(self, text: str) -> Optional[int]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²© ì¶”ì¶œ (ì› ë‹¨ìœ„)"""
        # ê°€ê²© íŒ¨í„´: 10,000ì›, 10000ì›, 1ë§Œì› ë“±
        patterns = [
            r'(\d{1,3}(?:,\d{3})*)\s*ì›',  # 10,000ì›
            r'(\d+)\s*ë§Œ\s*ì›',             # 1ë§Œì›
            r'(\d+)ì›',                      # 10000ì›
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                price_str = match.group(1).replace(',', '')
                if 'ë§Œ' in text[match.start():match.end()+2]:
                    return int(price_str) * 10000
                return int(price_str)

        return None

    def extract_time_from_text(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì‹œê°„ ì¶”ì¶œ (HH:MM-HH:MM í˜•ì‹)"""
        # ì‹œê°„ íŒ¨í„´: 06:00-08:00, 6ì‹œ-8ì‹œ, ì˜¤ì „ 6ì‹œ - ì˜¤ì „ 8ì‹œ
        time_ranges = []

        # HH:MM-HH:MM íŒ¨í„´
        pattern1 = r'(\d{1,2}):(\d{2})\s*[-~]\s*(\d{1,2}):(\d{2})'
        matches = re.findall(pattern1, text)
        for match in matches:
            start_time = f"{int(match[0]):02d}:{match[1]}"
            end_time = f"{int(match[2]):02d}:{match[3]}"
            time_ranges.append(f"{start_time}-{end_time}")

        # HHì‹œ-HHì‹œ íŒ¨í„´
        pattern2 = r'(\d{1,2})\s*ì‹œ\s*[-~]\s*(\d{1,2})\s*ì‹œ'
        matches = re.findall(pattern2, text)
        for match in matches:
            start_time = f"{int(match[0]):02d}:00"
            end_time = f"{int(match[1]):02d}:00"
            time_ranges.append(f"{start_time}-{end_time}")

        return time_ranges

    def crawl_pool_website(self, url: str, pool_name: str) -> Dict:
        """ìˆ˜ì˜ì¥ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê°€ê²© ë° ì‹œê°„ ì •ë³´ í¬ë¡¤ë§"""
        result = {
            "daily_price": None,
            "free_swim_price": None,
            "free_swim_times": [],
            "operating_hours": None
        }

        try:
            response = self.session.get(url, timeout=10)
            response.encoding = 'utf-8'

            if response.status_code != 200:
                return result

            soup = BeautifulSoup(response.text, 'html.parser')
            text = soup.get_text(separator=' ', strip=True)

            # ê°€ê²© ì •ë³´ ì¶”ì¶œ
            # "ììœ ìˆ˜ì˜" ê´€ë ¨ ì„¹ì…˜ ì°¾ê¸°
            free_swim_keywords = ['ììœ ìˆ˜ì˜', 'ììœ¨ìˆ˜ì˜', 'ì¼ì¼ì…ì¥', '1íšŒ ì´ìš©']
            lesson_keywords = ['ê°•ìŠµ', 'ìˆ˜ê°•', 'íšŒì›']

            # í…ìŠ¤íŠ¸ë¥¼ ì¤„ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
            lines = [line.strip() for line in text.split('\n') if line.strip()]

            for i, line in enumerate(lines):
                line_lower = line.lower()

                # ììœ ìˆ˜ì˜ ê°€ê²© ì°¾ê¸°
                if any(keyword in line for keyword in free_swim_keywords):
                    # í˜„ì¬ ì¤„ê³¼ ë‹¤ìŒ 3ì¤„ì—ì„œ ê°€ê²© ì°¾ê¸°
                    for j in range(i, min(i+4, len(lines))):
                        price = self.extract_price_from_text(lines[j])
                        if price and 3000 <= price <= 30000:  # í•©ë¦¬ì ì¸ ê°€ê²© ë²”ìœ„
                            if result["free_swim_price"] is None:
                                result["free_swim_price"] = price
                            break

                # ììœ ìˆ˜ì˜ ì‹œê°„ ì°¾ê¸°
                if any(keyword in line for keyword in free_swim_keywords):
                    for j in range(i, min(i+5, len(lines))):
                        times = self.extract_time_from_text(lines[j])
                        if times:
                            result["free_swim_times"].extend(times)

            # ì¼ì¼ê¶Œ ê°€ê²©ì´ ì—†ìœ¼ë©´ ììœ ìˆ˜ì˜ ê°€ê²© ì‚¬ìš©
            if result["free_swim_price"]:
                result["daily_price"] = result["free_swim_price"]

            # ì¤‘ë³µ ì œê±°
            if result["free_swim_times"]:
                result["free_swim_times"] = list(set(result["free_swim_times"]))

            return result

        except Exception as e:
            print(f"âœ— í¬ë¡¤ë§ ì‹¤íŒ¨ ({pool_name}): {str(e)[:50]}")
            return result

    def crawl_all_pools(self):
        """DBì˜ ëª¨ë“  ìˆ˜ì˜ì¥ ê°€ê²© ì •ë³´ í¬ë¡¤ë§"""
        conn = sqlite3.connect('swimming_pools.db')
        cursor = conn.cursor()

        # URLì´ ì—†ê±°ë‚˜ ê°€ê²©ì´ ê¸°ë³¸ê°’ì¸ ìˆ˜ì˜ì¥ ê°€ì ¸ì˜¤ê¸°
        cursor.execute('''
            SELECT id, name, address, url
            FROM swimming_pools
            WHERE (url IS NULL OR url = '')
               OR (daily_price = 10000 OR daily_price = 5000)
            ORDER BY id
        ''')

        pools = cursor.fetchall()
        total = len(pools)

        print(f"\nğŸ” {total}ê°œ ìˆ˜ì˜ì¥ ê°€ê²© í¬ë¡¤ë§ ì‹œì‘...\n")

        success_count = 0
        failed_count = 0

        for i, (pool_id, name, address, url) in enumerate(pools):
            print(f"[{i+1}/{total}] {name}")

            # 1ë‹¨ê³„: ì›¹ì‚¬ì´íŠ¸ ì°¾ê¸°
            if not url or url == '':
                print(f"  â†’ ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì¤‘...")
                url = self.find_pool_website(name, address)

                if url:
                    print(f"  âœ“ ì›¹ì‚¬ì´íŠ¸ ë°œê²¬: {url[:50]}...")
                    # URL ì—…ë°ì´íŠ¸
                    cursor.execute('UPDATE swimming_pools SET url = ? WHERE id = ?', (url, pool_id))
                else:
                    print(f"  âœ— ì›¹ì‚¬ì´íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    failed_count += 1
                    continue

            # 2ë‹¨ê³„: ê°€ê²© ì •ë³´ í¬ë¡¤ë§
            print(f"  â†’ ê°€ê²© ì •ë³´ í¬ë¡¤ë§ ì¤‘...")
            price_data = self.crawl_pool_website(url, name)

            # 3ë‹¨ê³„: DB ì—…ë°ì´íŠ¸ (ê°’ì´ ìˆëŠ” ê²½ìš°ë§Œ)
            updates = []
            params = []

            if price_data["daily_price"]:
                updates.append("daily_price = ?")
                params.append(price_data["daily_price"])
                print(f"  âœ“ ì¼ì¼ê¶Œ: {price_data['daily_price']:,}ì›")

            if price_data["free_swim_price"]:
                updates.append("free_swim_price = ?")
                params.append(price_data["free_swim_price"])
                print(f"  âœ“ ììœ ìˆ˜ì˜: {price_data['free_swim_price']:,}ì›")

            if price_data["free_swim_times"]:
                updates.append("free_swim_times = ?")
                params.append(json.dumps({"times": price_data["free_swim_times"]}, ensure_ascii=False))
                print(f"  âœ“ ììœ ìˆ˜ì˜ ì‹œê°„: {', '.join(price_data['free_swim_times'][:3])}")

            if updates:
                params.append(pool_id)
                query = f"UPDATE swimming_pools SET {', '.join(updates)} WHERE id = ?"
                cursor.execute(query, params)
                success_count += 1
                print(f"  âœ“ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            else:
                print(f"  âœ— ê°€ê²© ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                failed_count += 1

            conn.commit()
            time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            print()

        conn.close()

        print(f"\n{'='*60}")
        print(f"âœ… í¬ë¡¤ë§ ì™„ë£Œ")
        print(f"  â€¢ ì„±ê³µ: {success_count}ê°œ")
        print(f"  â€¢ ì‹¤íŒ¨: {failed_count}ê°œ")
        print(f"{'='*60}\n")

if __name__ == "__main__":
    print("ğŸŠ ìˆ˜ì˜ì¥ ê°€ê²© í¬ë¡¤ëŸ¬\n")

    crawler = PoolPriceCrawler()
    crawler.crawl_all_pools()
