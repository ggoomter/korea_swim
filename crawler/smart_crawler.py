# -*- coding: utf-8 -*-
"""
ìŠ¤ë§ˆíŠ¸ ìˆ˜ì˜ì¥ í¬ë¡¤ëŸ¬ - ChatGPT ê°€ì´ë“œ ì ìš©
- ì„¸ì…˜ ì¬ì‚¬ìš© + Retry + ì§€ìˆ˜ ë°±ì˜¤í”„
- robots.txt í™•ì¸
- User-Agent ìœ„ì¥
- ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê°€ê²© ì •ë³´ í¬ë¡¤ë§
"""
import sys
import io
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')

import time
import random
import re
from urllib.parse import urljoin, urlparse
from typing import List, Dict, Optional
import urllib.robotparser as rp

import requests
from requests.adapters import HTTPAdapter, Retry
from bs4 import BeautifulSoup
import json

class SmartPoolCrawler:
    def __init__(self):
        # ì„¸ì…˜ + Retry ì„¤ì •
        self.session = requests.Session()
        retries = Retry(
            total=5,
            backoff_factor=0.5,  # 0.5ì´ˆ, 1ì´ˆ, 2ì´ˆ, 4ì´ˆ, 8ì´ˆë¡œ ì¦ê°€
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["GET", "POST"]
        )
        self.session.mount("https://", HTTPAdapter(max_retries=retries))
        self.session.mount("http://", HTTPAdapter(max_retries=retries))

        # User-Agent ìœ„ì¥
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
            "Accept-Language": "ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        })

        self.robots_cache = {}  # robots.txt ìºì‹œ

    def can_fetch(self, url: str, user_agent: str = "*") -> bool:
        """robots.txt í™•ì¸"""
        try:
            parsed = urlparse(url)
            base_url = f"{parsed.scheme}://{parsed.netloc}"

            if base_url not in self.robots_cache:
                robots_url = f"{base_url}/robots.txt"
                parser = rp.RobotFileParser()
                parser.set_url(robots_url)
                parser.read()
                self.robots_cache[base_url] = parser

            return self.robots_cache[base_url].can_fetch(user_agent, url)
        except Exception:
            # robots.txt ì—†ìœ¼ë©´ í—ˆìš©
            return True

    def get(self, url: str, **kwargs) -> Optional[requests.Response]:
        """ì„¸ì…˜ì„ ì‚¬ìš©í•œ GET ìš”ì²­"""
        try:
            # robots.txt í™•ì¸
            if not self.can_fetch(url):
                print(f"âŒ robots.txt ì°¨ë‹¨: {url}")
                return None

            # íƒ€ì„ì•„ì›ƒ ê¸°ë³¸ê°’ ì„¤ì •
            kwargs.setdefault("timeout", 10)

            # ìš”ì²­ ì „ ëŒ€ê¸° (ì˜ˆì˜)
            time.sleep(random.uniform(0.5, 1.5))

            response = self.session.get(url, **kwargs)
            response.raise_for_status()
            return response
        except requests.exceptions.RequestException as e:
            print(f"âŒ ìš”ì²­ ì‹¤íŒ¨: {url} - {str(e)[:50]}")
            return None

    def extract_price_from_text(self, text: str) -> Optional[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ê°€ê²© ì •ë³´ ì¶”ì¶œ"""
        if not text:
            return None

        # íŒ¨í„´ 1: "000ì›", "0,000ì›"
        patterns = [
            r'(\d{1,3}(?:,\d{3})*)\s*ì›',
            r'(\d{4,})\s*ì›',
            r'â‚©\s*(\d{1,3}(?:,\d{3})*)',
            r'(\d{1,3}(?:,\d{3})*)\s*KRW',
        ]

        for pattern in patterns:
            match = re.search(pattern, text)
            if match:
                price = match.group(1).replace(',', '')
                return price

        # íŠ¹ìˆ˜ ì¼€ì´ìŠ¤: "ë¬´ë£Œ", "ë¬¸ì˜"
        if any(keyword in text for keyword in ['ë¬´ë£Œ', 'ê³µì§œ']):
            return 'ë¬´ë£Œ'
        if any(keyword in text for keyword in ['ë¬¸ì˜', 'ë³„ë„', 'ìƒë‹´']):
            return 'ë¬¸ì˜'

        return None

    def crawl_gangnam_pool(self, url: str, pool_name: str) -> Dict:
        """ê°•ë‚¨êµ¬ì²­ ì²´ìœ¡ì‹œì„¤ í˜ì´ì§€ í¬ë¡¤ë§"""
        result = {"url": url, "monthly_lesson_price": None, "free_swim_price": None}

        response = self.get(url)
        if not response:
            return result

        soup = BeautifulSoup(response.text, "html.parser")

        # ê°•ë‚¨êµ¬ì²­ ì‚¬ì´íŠ¸ êµ¬ì¡°ì— ë§ê²Œ ì…€ë ‰í„° ì‘ì„±
        # ì˜ˆ: ê°€ê²©í‘œê°€ í…Œì´ë¸”ì— ìˆëŠ” ê²½ìš°
        tables = soup.find_all("table")
        for table in tables:
            text = table.get_text()

            # "ìˆ˜ê°•ë£Œ", "ê°•ìŠµë£Œ", "ì´ìš©ë£Œ" ë“± í‚¤ì›Œë“œ ì°¾ê¸°
            if any(keyword in text for keyword in ['ìˆ˜ê°•ë£Œ', 'ê°•ìŠµë£Œ', 'ì´ìš©ë£Œ', 'íšŒë¹„']):
                # í•œë‹¬ ìˆ˜ê°•ê¶Œ
                if '1ê°œì›”' in text or 'í•œë‹¬' in text or 'ì›”' in text:
                    price = self.extract_price_from_text(text)
                    if price:
                        result['monthly_lesson_price'] = price

                # ììœ ìˆ˜ì˜
                if 'ììœ ìˆ˜ì˜' in text or 'ììœ¨ìˆ˜ì˜' in text:
                    price = self.extract_price_from_text(text)
                    if price:
                        result['free_swim_price'] = price

        # ë¦¬ìŠ¤íŠ¸ì—ì„œ ì°¾ê¸°
        price_elements = soup.select(".price, .fee, .cost, dl, .info-list")
        for elem in price_elements:
            text = elem.get_text()
            if 'ìˆ˜ì˜' in text or 'ê°•ìŠµ' in text:
                price = self.extract_price_from_text(text)
                if price and not result['monthly_lesson_price']:
                    result['monthly_lesson_price'] = price

        return result

    def crawl_generic_pool(self, url: str, pool_name: str) -> Dict:
        """ì¼ë°˜ì ì¸ ìˆ˜ì˜ì¥ ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ë²”ìš©)"""
        result = {"url": url, "monthly_lesson_price": None, "free_swim_price": None}

        response = self.get(url)
        if not response:
            return result

        soup = BeautifulSoup(response.text, "html.parser")

        # ì „ëµ 1: ë©”íƒ€ íƒœê·¸ì—ì„œ ì°¾ê¸°
        meta_desc = soup.find("meta", {"name": "description"})
        if meta_desc and meta_desc.get("content"):
            price = self.extract_price_from_text(meta_desc["content"])
            if price:
                result['monthly_lesson_price'] = price

        # ì „ëµ 2: ê°€ê²© ê´€ë ¨ í´ë˜ìŠ¤/ID ì°¾ê¸°
        price_selectors = [
            ".price", ".fee", ".cost", "#price", "#fee",
            "[class*='price']", "[class*='fee']", "[id*='price']"
        ]

        for selector in price_selectors:
            elements = soup.select(selector)
            for elem in elements:
                text = elem.get_text()
                if 'ìˆ˜ì˜' in text or 'ê°•ìŠµ' in text or 'ì´ìš©' in text:
                    price = self.extract_price_from_text(text)
                    if price:
                        if 'ììœ ' in text or 'ììœ¨' in text:
                            result['free_swim_price'] = price
                        else:
                            result['monthly_lesson_price'] = price

        # ì „ëµ 3: ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ íŒ¨í„´ ë§¤ì¹­
        full_text = soup.get_text()

        # "ìˆ˜ê°•ë£Œ: 150,000ì›" í˜•íƒœ
        lesson_match = re.search(r'(?:ìˆ˜ê°•ë£Œ|ê°•ìŠµë£Œ|íšŒë¹„)[:\s]*(\d{1,3}(?:,\d{3})*)\s*ì›', full_text)
        if lesson_match:
            result['monthly_lesson_price'] = lesson_match.group(1).replace(',', '')

        # "ììœ ìˆ˜ì˜: 8,000ì›" í˜•íƒœ
        swim_match = re.search(r'(?:ììœ ìˆ˜ì˜|ììœ¨ìˆ˜ì˜)[:\s]*(\d{1,3}(?:,\d{3})*)\s*ì›', full_text)
        if swim_match:
            result['free_swim_price'] = swim_match.group(1).replace(',', '')

        return result

    def crawl_pool_website(self, url: str, pool_name: str) -> Dict:
        """ìˆ˜ì˜ì¥ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ (ë„ë©”ì¸ë³„ ì „ëµ)"""
        if not url or url == "ì •ë³´ ì—†ìŒ":
            return {"url": None, "monthly_lesson_price": None, "free_swim_price": None}

        print(f"ğŸ” í¬ë¡¤ë§: {pool_name}")
        print(f"   URL: {url}")

        # URL ì •ê·œí™”
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url

        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()

            # ë„ë©”ì¸ë³„ ë§ì¶¤ í¬ë¡¤ëŸ¬
            if 'gangnam.go.kr' in domain:
                result = self.crawl_gangnam_pool(url, pool_name)
            elif 'gangdong.go.kr' in domain or 'gangbuk.go.kr' in domain:
                result = self.crawl_gangnam_pool(url, pool_name)  # ë¹„ìŠ·í•œ êµ¬ì¡°
            else:
                result = self.crawl_generic_pool(url, pool_name)

            # ê²°ê³¼ ì¶œë ¥
            if result['monthly_lesson_price'] or result['free_swim_price']:
                print(f"   âœ… ìˆ˜ê°•ê¶Œ: {result['monthly_lesson_price'] or 'ì •ë³´ì—†ìŒ'}")
                print(f"   âœ… ììœ ìˆ˜ì˜: {result['free_swim_price'] or 'ì •ë³´ì—†ìŒ'}")
            else:
                print(f"   âš ï¸  ê°€ê²© ì •ë³´ ì—†ìŒ")

            return result

        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜: {str(e)[:50]}")
            return {"url": url, "monthly_lesson_price": None, "free_swim_price": None}

    def crawl_pools_from_db(self, limit: int = 10) -> List[Dict]:
        """DBì—ì„œ ìˆ˜ì˜ì¥ ëª©ë¡ ê°€ì ¸ì™€ì„œ í¬ë¡¤ë§"""
        import sys
        import os
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

        from database.connection import get_db
        from app.models.swimming_pool import SwimmingPool

        db = next(get_db())

        # URLì´ ìˆëŠ” ìˆ˜ì˜ì¥ë§Œ
        pools = db.query(SwimmingPool).filter(
            SwimmingPool.url.isnot(None),
            SwimmingPool.url != '',
            SwimmingPool.url != 'ì •ë³´ ì—†ìŒ'
        ).limit(limit).all()

        results = []

        print(f"\n{'='*70}")
        print(f"  ğŸŠ {len(pools)}ê°œ ìˆ˜ì˜ì¥ ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì‹œì‘")
        print(f"{'='*70}\n")

        for i, pool in enumerate(pools, 1):
            print(f"\n[{i}/{len(pools)}] {pool.name}")

            result = self.crawl_pool_website(pool.url, pool.name)
            result['pool_id'] = pool.id
            result['pool_name'] = pool.name
            results.append(result)

            # ì˜ˆì˜ ì§€í‚¤ê¸°
            time.sleep(random.uniform(1.0, 2.0))

        return results

    def update_db_with_results(self, results: List[Dict]):
        """í¬ë¡¤ë§ ê²°ê³¼ë¡œ DB ì—…ë°ì´íŠ¸"""
        import sys
        import os
        sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

        from database.connection import get_db
        from app.models.swimming_pool import SwimmingPool

        db = next(get_db())

        updated_count = 0

        for result in results:
            if not result.get('pool_id'):
                continue

            pool = db.query(SwimmingPool).filter(
                SwimmingPool.id == result['pool_id']
            ).first()

            if not pool:
                continue

            updated = False

            # ê°€ê²© ì •ë³´ ì—…ë°ì´íŠ¸ (ë¬¸ìì—´ë¡œ ì €ì¥)
            if result.get('monthly_lesson_price'):
                pool.monthly_lesson_price = str(result['monthly_lesson_price'])
                updated = True

            if result.get('free_swim_price'):
                pool.free_swim_price = str(result['free_swim_price'])
                updated = True

            if updated:
                updated_count += 1

        db.commit()

        print(f"\n{'='*70}")
        print(f"  âœ… {updated_count}ê°œ ìˆ˜ì˜ì¥ ê°€ê²© ì •ë³´ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
        print(f"{'='*70}")

        return updated_count


def main():
    print("\n" + "="*70)
    print("  ğŸŠ ìŠ¤ë§ˆíŠ¸ ìˆ˜ì˜ì¥ í¬ë¡¤ëŸ¬ (ChatGPT ê°€ì´ë“œ ì ìš©)")
    print("="*70)

    crawler = SmartPoolCrawler()

    # ìƒ˜í”Œ: ì²˜ìŒ 10ê°œë§Œ í…ŒìŠ¤íŠ¸
    results = crawler.crawl_pools_from_db(limit=10)

    # ê²°ê³¼ ì €ì¥
    with open('crawl_results.json', 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥: crawl_results.json")

    # DB ì—…ë°ì´íŠ¸
    updated = crawler.update_db_with_results(results)

    print(f"\nğŸ“Š í†µê³„:")
    print(f"   - ì‹œë„: {len(results)}ê°œ")
    print(f"   - ì„±ê³µ: {sum(1 for r in results if r.get('monthly_lesson_price') or r.get('free_swim_price'))}ê°œ")
    print(f"   - ì—…ë°ì´íŠ¸: {updated}ê°œ")


if __name__ == "__main__":
    main()
